{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by [Gameli Ladzekpo](mailto:gameli.Ladzekpo@gmail.com) (Twitter/IG: @gamladz)\n",
    "\n",
    "For [AI Core](theaicore.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train.csv') \n",
    "y_train = pd.read_csv('y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             prod_price   R-squared:                       0.260\nModel:                            OLS   Adj. R-squared:                  0.238\nMethod:                 Least Squares   F-statistic:                     12.05\nDate:                Wed, 24 Feb 2021   Prob (F-statistic):           5.08e-21\nTime:                        13:03:22   Log-Likelihood:                -2388.3\nNo. Observations:                 425   AIC:                             4803.\nDf Residuals:                     412   BIC:                             4855.\nDf Model:                          12                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst           12.3608      5.239      2.359      0.019       2.061      22.660\nvolume_avg    6.039e-14   1.49e-14      4.062      0.000    3.12e-14    8.96e-14\nlength_avg    1.718e-14   5.15e-15      3.336      0.001    7.06e-15    2.73e-14\ndiameter_avg    -1.0922      0.616     -1.774      0.077      -2.303       0.118\ndepth_avg       -0.8136      1.334     -0.610      0.542      -3.436       1.809\nwidth_avg       -0.1997      0.627     -0.318      0.750      -1.433       1.034\nheight_avg    1.681e-15   8.46e-15      0.199      0.843   -1.49e-14    1.83e-14\nis_plant         0.0504      9.027      0.006      0.996     -17.695      17.795\nin_outdoor     -21.5423     15.131     -1.424      0.155     -51.287       8.202\nis_white        97.4228      9.093     10.714      0.000      79.549     115.297\nis_grey         22.4547     10.649      2.109      0.036       1.521      43.389\nis_beige        47.1551     18.137      2.600      0.010      11.502      82.808\nis_black        -0.4298     21.403     -0.020      0.984     -42.502      41.643\nis_pink          1.0074     26.869      0.037      0.970     -51.810      53.825\nis_blue         -1.5164     22.132     -0.069      0.945     -45.022      41.989\nis_red          -8.5256     23.222     -0.367      0.714     -54.175      37.123\n==============================================================================\nOmnibus:                      354.484   Durbin-Watson:                   1.938\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             8130.115\nSkew:                           3.461   Prob(JB):                         0.00\nKurtosis:                      23.278   Cond. No.                     5.76e+17\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 6.75e-32. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "model = sm.OLS(y_train, X_train.astype(float)).fit()\n",
    "predictions = model.predict(X_train) \n",
    "\n",
    "print_model = model.summary()\n",
    "print(print_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             prod_price   R-squared:                       0.260\nModel:                            OLS   Adj. R-squared:                  0.246\nMethod:                 Least Squares   F-statistic:                     18.25\nDate:                Wed, 24 Feb 2021   Prob (F-statistic):           1.87e-23\nTime:                        15:43:46   Log-Likelihood:                -2388.3\nNo. Observations:                 425   AIC:                             4795.\nDf Residuals:                     416   BIC:                             4831.\nDf Model:                           8                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst           12.3262      4.148      2.971      0.003       4.172      20.480\nvolume_avg   -5.968e-14   8.22e-15     -7.258      0.000   -7.58e-14   -4.35e-14\ndiameter_avg    -1.0962      0.603     -1.819      0.070      -2.281       0.088\ndepth_avg       -0.8135      1.327     -0.613      0.540      -3.423       1.796\nwidth_avg       -0.1997      0.624     -0.320      0.749      -1.427       1.028\nin_outdoor     -21.4439     14.421     -1.487      0.138     -49.792       6.904\nis_white        97.4661      8.611     11.319      0.000      80.540     114.392\nis_grey         22.4897     10.279      2.188      0.029       2.284      42.695\nis_beige        47.1417     17.799      2.649      0.008      12.155      82.128\nis_red          -8.3768     22.831     -0.367      0.714     -53.256      36.503\n==============================================================================\nOmnibus:                      354.460   Durbin-Watson:                   1.937\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             8128.182\nSkew:                           3.461   Prob(JB):                         0.00\nKurtosis:                      23.276   Cond. No.                     3.95e+16\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 1.44e-29. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.drop('width_avg', axis=1)\n",
    "model = sm.OLS(y_train, X_train.astype(float)).fit()\n",
    "predictions = model.predict(X_train) \n",
    "\n",
    "print_model = model.summary()\n",
    "print(print_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.25984503397938385\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = LinearRegression()  \n",
    "model.fit(X_train, y_train)  \n",
    "y_pred = model.predict(X_train)  \n",
    "score = model.score(X_train, y_train)\n",
    "\n",
    "\n",
    "print(score)\n",
    "\n",
    "\n",
    "# Drop Columns\n",
    "# Regularisation --> Look at L1 and L2 regularisation and use Alpha\n",
    "# Decision Tree, hyperparameter optimisation with Grid search CV\n",
    "# Model Interpretation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'adjusted_mutual_info_score',\n",
       " 'adjusted_rand_score',\n",
       " 'average_precision',\n",
       " 'balanced_accuracy',\n",
       " 'completeness_score',\n",
       " 'explained_variance',\n",
       " 'f1',\n",
       " 'f1_macro',\n",
       " 'f1_micro',\n",
       " 'f1_samples',\n",
       " 'f1_weighted',\n",
       " 'fowlkes_mallows_score',\n",
       " 'homogeneity_score',\n",
       " 'jaccard',\n",
       " 'jaccard_macro',\n",
       " 'jaccard_micro',\n",
       " 'jaccard_samples',\n",
       " 'jaccard_weighted',\n",
       " 'max_error',\n",
       " 'mutual_info_score',\n",
       " 'neg_brier_score',\n",
       " 'neg_log_loss',\n",
       " 'neg_mean_absolute_error',\n",
       " 'neg_mean_gamma_deviance',\n",
       " 'neg_mean_poisson_deviance',\n",
       " 'neg_mean_squared_error',\n",
       " 'neg_mean_squared_log_error',\n",
       " 'neg_median_absolute_error',\n",
       " 'neg_root_mean_squared_error',\n",
       " 'normalized_mutual_info_score',\n",
       " 'precision',\n",
       " 'precision_macro',\n",
       " 'precision_micro',\n",
       " 'precision_samples',\n",
       " 'precision_weighted',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'recall_macro',\n",
       " 'recall_micro',\n",
       " 'recall_samples',\n",
       " 'recall_weighted',\n",
       " 'roc_auc',\n",
       " 'roc_auc_ovo',\n",
       " 'roc_auc_ovo_weighted',\n",
       " 'roc_auc_ovr',\n",
       " 'roc_auc_ovr_weighted',\n",
       " 'v_measure_score']"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best Score: 0.1179956622205926\nBest Hyperparameters: {'alpha': 61.679369216771114, 'fit_intercept': False, 'normalize': True, 'solver': 'lsqr'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.stats import loguniform\n",
    "from pandas import read_csv\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "model = Ridge() \n",
    "# model.fit(X_train, y_train)  \n",
    "# y_pred = model.predict(X_train)  \n",
    "# score = model.score(X_train, y_train)\n",
    "\n",
    "# define evaluation\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define search space\n",
    "space = dict()\n",
    "space['solver'] = ['svd', 'cholesky', 'lsqr', 'sag']\n",
    "space['solver'] = ['lsqr']\n",
    "space['alpha'] = loguniform(1e-5, 100)\n",
    "space['fit_intercept'] = [True, False]\n",
    "space['normalize'] = [True, False]\n",
    "# define search\n",
    "search = RandomizedSearchCV(model, space, n_iter=300, scoring='r2', n_jobs=-1, cv=cv, random_state=1)\n",
    "\n",
    "result = search.fit(X_train, y_train)\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)\n",
    "\n",
    "\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.2596116243595519\n"
     ]
    }
   ],
   "source": [
    "model = linear_model.Lasso(alpha=0.1) \n",
    "model.fit(X_train, y_train) \n",
    "y_pred = model.predict(X_train)  # make predictions\n",
    "score = model.score(X_train, y_train)\n",
    "\n",
    "\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.30689188938731093\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regressor\n",
    "\n",
    "model = DecisionTreeRegressor(max_depth=2)\n",
    "model.fit(X_train, y_train) \n",
    "y_pred = model.predict(X_train)  # make predictions\n",
    "score = model.score(X_train, y_train)\n",
    "\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.3520168856173721\n"
     ]
    }
   ],
   "source": [
    "# Elastic net\n",
    "\n",
    "polynomial_features= PolynomialFeatures(degree=2)\n",
    "x_poly = polynomial_features.fit_transform(X_train)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_poly, y_train)\n",
    "y_pred = model.predict(x_poly)\n",
    "score = model.score(x_poly, y_train)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best Score: 0.06905295025351858\nBest Hyperparameters: {'alpha': 5.222106944089181, 'fit_intercept': True, 'normalize': True, 'solver': 'lsqr'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.stats import loguniform\n",
    "from pandas import read_csv\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "polynomial_features= PolynomialFeatures(degree=2)\n",
    "x_poly = polynomial_features.fit_transform(X_train)\n",
    "\n",
    "\n",
    "model = Ridge() \n",
    "# model.fit(X_train, y_train)  \n",
    "# y_pred = model.predict(X_train)  \n",
    "# score = model.score(X_train, y_train)\n",
    "\n",
    "# define evaluation\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define search space\n",
    "space = dict()\n",
    "space['solver'] = ['svd', 'cholesky', 'lsqr', 'sag']\n",
    "space['solver'] = ['lsqr']\n",
    "space['alpha'] = loguniform(1e-5, 100)\n",
    "space['fit_intercept'] = [True, False]\n",
    "space['normalize'] = [True, False]\n",
    "\n",
    "# define search\n",
    "search = RandomizedSearchCV(model, space, n_iter=300, scoring='r2', n_jobs=-1, cv=cv, random_state=1)\n",
    "result = search.fit(x_poly, y_train)\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)\n",
    "\n",
    "\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}